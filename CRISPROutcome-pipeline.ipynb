{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.SeqUtils import gc_fraction\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqUtils import MeltingTemp\n",
    "from scipy.stats import entropy\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import warnings\n",
    "import json\n",
    "from collections import Counter\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv') #import your training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRISPRFeaturePipeline:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_columns = None\n",
    "        \n",
    "    class CRISPRFeatureExtractor:\n",
    "        def __init__(self, sequence):\n",
    "            self.sequence = sequence.upper()\n",
    "            self.guide = self.sequence[:-3]  # 20nt guide sequence\n",
    "            self.pam = self.sequence[-3:]    # 3bp PAM\n",
    "            \n",
    "            # Define all possible dinucleotides\n",
    "            self.all_dinucs = ['AA', 'AC', 'AG', 'AT', 'CA', 'CC', 'CG', 'CT', \n",
    "                              'GA', 'GC', 'GG', 'GT', 'TA', 'TC', 'TG', 'TT']\n",
    "            \n",
    "            # Cut site is typically between positions 17 and 18 of guide\n",
    "            self.cut_site_index = len(self.guide) - 3\n",
    "            \n",
    "        def get_position_features(self):\n",
    "            \"\"\"One-hot encode guide sequence and PAM\"\"\"\n",
    "            nucleotides = ['A', 'T', 'C', 'G']\n",
    "            features = {}\n",
    "            \n",
    "            # One-hot encode guide sequence\n",
    "            for i, nt in enumerate(self.guide):\n",
    "                for n in nucleotides:\n",
    "                    features[f'guide_pos_{i}_{n}'] = 1 if nt == n else 0\n",
    "                    \n",
    "            # One-hot encode PAM\n",
    "            for i, nt in enumerate(self.pam):\n",
    "                for n in nucleotides:\n",
    "                    features[f'pam_pos_{i}_{n}'] = 1 if nt == n else 0\n",
    "                    \n",
    "            return features\n",
    "        \n",
    "        def get_cut_site_context(self):\n",
    "            \"\"\"Get features around cut site\"\"\"\n",
    "            features = {}\n",
    "            cut_start = max(0, self.cut_site_index - 3)\n",
    "            cut_end = min(len(self.guide), self.cut_site_index + 4)\n",
    "            cut_context = self.guide[cut_start:cut_end]\n",
    "            \n",
    "            nucleotides = ['A', 'T', 'C', 'G']\n",
    "            for i, nt in enumerate(cut_context):\n",
    "                pos = i - 3  # Position relative to cut site\n",
    "                for n in nucleotides:\n",
    "                    features[f'cut_site_{pos}_{n}'] = 1 if nt == n else 0\n",
    "                    \n",
    "            return features\n",
    "        \n",
    "        \n",
    "        def get_sequence_features(self):\n",
    "            \"\"\"Get sequence features\"\"\"\n",
    "            features = {}\n",
    "            \n",
    "            features['gc_content'] = gc_fraction(self.guide)\n",
    "            \n",
    "            for dinuc in self.all_dinucs:\n",
    "                features[f'dinuc_{dinuc}'] = 0\n",
    "                \n",
    "            for i in range(len(self.guide)-1):\n",
    "                dinuc = self.guide[i:i+2]\n",
    "                features[f'dinuc_{dinuc}'] += 1\n",
    "            \n",
    "            return features\n",
    "        \n",
    "        def get_all_features(self):\n",
    "            \"\"\"Combine all features\"\"\"\n",
    "            features = {}\n",
    "            features.update(self.get_sequence_features())\n",
    "            features.update(self.get_position_features())\n",
    "            features.update(self.get_cut_site_context())\n",
    "            return features\n",
    "    \n",
    "    def extract_features(self, df, fit_scaler=True):\n",
    "        \"\"\"\n",
    "        Extract features from DataFrame and optionally scale them\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with 'Id' and 'GuideSeq' columns\n",
    "            fit_scaler: Whether to fit the scaler (True for training, False for test)\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with extracted features and original Id\n",
    "        \"\"\"\n",
    "        # Extract features\n",
    "        features_list = []\n",
    "        ids = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            extractor = self.CRISPRFeatureExtractor(row['GuideSeq'])\n",
    "            features = extractor.get_all_features()\n",
    "            features_list.append(features)\n",
    "            ids.append(row['Id'])\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        feature_df = pd.DataFrame(features_list)\n",
    "        feature_df['Id'] = ids\n",
    "        \n",
    "        # Store feature columns if not already stored\n",
    "        if self.feature_columns is None:\n",
    "            self.feature_columns = [col for col in feature_df.columns if col != 'Id']\n",
    "        \n",
    "        # Scale features\n",
    "        if fit_scaler:\n",
    "            scaled_features = self.scaler.fit_transform(feature_df[self.feature_columns])\n",
    "        else:\n",
    "            scaled_features = self.scaler.transform(feature_df[self.feature_columns])\n",
    "            \n",
    "        scaled_df = pd.DataFrame(scaled_features, columns=self.feature_columns)\n",
    "        scaled_df['Id'] = ids\n",
    "        \n",
    "        return scaled_df\n",
    "    \n",
    "    def process_data(self, df, is_training=True):\n",
    "        \"\"\"\n",
    "        Process data and merge with original DataFrame if training\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            is_training: Whether this is training data (True) or test data (False)\n",
    "        \n",
    "        Returns:\n",
    "            Processed DataFrame\n",
    "        \"\"\"\n",
    "        # Extract and scale features\n",
    "        features_df = self.extract_features(df, fit_scaler=is_training)\n",
    "        \n",
    "        if is_training:\n",
    "            # For training data, merge with original targets\n",
    "            targets = ['Fraction_Insertions', 'Avg_Deletion_Length',\n",
    "                      'Indel_Diversity', 'Fraction_Frameshifts']\n",
    "            result_df = pd.merge(features_df, \n",
    "                               df[['Id'] + targets],\n",
    "                               on='Id')\n",
    "        else:\n",
    "            # For test data, just return features\n",
    "            result_df = features_df\n",
    "            \n",
    "        print(f\"Processed {len(result_df)} sequences\")\n",
    "        print(f\"Total features: {len(self.feature_columns)}\")\n",
    "        print(f\"Missing values: {result_df.isnull().sum().sum()}\")\n",
    "        \n",
    "        return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pipeline\n",
    "pipeline = CRISPRFeaturePipeline()\n",
    "\n",
    "# Process training data\n",
    "train_processed = pipeline.process_data(train_df, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_processed is the DataFrame\n",
    "train_processed = train_processed.set_index('Id').reset_index()\n",
    "\n",
    "# OR alternatively:\n",
    "cols = ['Id'] + [col for col in train_processed.columns if col != 'Id']\n",
    "train_processed = train_processed[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define targets\n",
    "targets = ['Fraction_Insertions', 'Avg_Deletion_Length', \n",
    "           'Indel_Diversity', 'Fraction_Frameshifts']\n",
    "\n",
    "# Get feature columns\n",
    "feature_cols = [col for col in train_processed.columns \n",
    "               if col not in targets + ['Id']]\n",
    "\n",
    "# Split data\n",
    "X = train_processed[feature_cols]\n",
    "y = train_processed[targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 6,\n",
    "    'num_leaves': 31,\n",
    "    'min_child_samples': 20,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42,\n",
    "    'verbose': -1,\n",
    "    'metric': 'r2'\n",
    "}\n",
    "\n",
    "# Train models for each target\n",
    "models = {}\n",
    "results = {}\n",
    "\n",
    "for target in targets:\n",
    "   print(f\"\\nTraining model for {target}\")\n",
    "   \n",
    "   model = lgb.LGBMRegressor(**params)\n",
    "   \n",
    "   # Cross validation\n",
    "   scores = cross_val_score(model, X_train, y_train[target], cv=5, scoring='r2')\n",
    "   print(f\"CV R² scores: {np.mean(scores):.4f} ± {np.std(scores):.4f}\")\n",
    "   \n",
    "   # Train final model\n",
    "   model.fit(\n",
    "       X_train, \n",
    "       y_train[target],\n",
    "       eval_set=[(X_train, y_train[target]), (X_val, y_val[target])],\n",
    "       eval_metric='r2'\n",
    "   )\n",
    "   \n",
    "   # Get R2 scores\n",
    "   train_r2 = model.score(X_train, y_train[target])\n",
    "   val_r2 = model.score(X_val, y_val[target])\n",
    "   \n",
    "   # Store results\n",
    "   results[target] = {\n",
    "       'train_r2': train_r2,\n",
    "       'val_r2': val_r2,\n",
    "       'cv_mean': np.mean(scores),\n",
    "       'cv_std': np.std(scores)\n",
    "   }\n",
    "   \n",
    "   models[target] = model\n",
    "\n",
    "# Print summary of all results\n",
    "print(\"\\nSummary of model performance:\")\n",
    "for target, metrics in results.items():\n",
    "   print(f\"\\n{target}:\")\n",
    "   print(f\"Training R²: {metrics['train_r2']:.4f}\")\n",
    "   print(f\"Validation R²: {metrics['val_r2']:.4f}\")\n",
    "   print(f\"CV R²: {metrics['cv_mean']:.4f} ± {metrics['cv_std']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100,200,300, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'num_leaves': [15, 31, 63],\n",
    "    'min_child_samples': [10, 20, 30]\n",
    "}\n",
    "\n",
    "# Base model configuration\n",
    "base_params = {\n",
    "    'objective': 'regression',\n",
    "    'random_state': 42,\n",
    "    'verbose': -1,\n",
    "    'metric': 'r2'\n",
    "}\n",
    "\n",
    "# Grid search for each target\n",
    "models = {}\n",
    "for target in targets:\n",
    "    print(f\"\\nTuning model for {target}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = lgb.LGBMRegressor(**base_params)\n",
    "    \n",
    "    # Grid search with cross validation\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring='r2',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit grid search\n",
    "    grid_search.fit(X_train, y_train[target])\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Best R² score: {grid_search.best_score_:.4f}\")\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    final_model = lgb.LGBMRegressor(**base_params, **grid_search.best_params_)\n",
    "    final_model.fit(\n",
    "        X_train,\n",
    "        y_train[target],\n",
    "        eval_set=[(X_val, y_val[target])],\n",
    "        eval_metric='r2'\n",
    "    )\n",
    "    \n",
    "    models[target] = final_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "# Create target-specific base models based on tuning results\n",
    "base_models = {\n",
    "    'Fraction_Insertions': lgb.LGBMRegressor(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        num_leaves=15,\n",
    "        min_child_samples=20,\n",
    "        subsample=0.8,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    ),\n",
    "    'Avg_Deletion_Length': lgb.LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=4,\n",
    "        num_leaves=15,\n",
    "        min_child_samples=30,\n",
    "        subsample=0.8,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    ),\n",
    "    'Indel_Diversity': lgb.LGBMRegressor(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=6,\n",
    "        num_leaves=15,\n",
    "        min_child_samples=10,\n",
    "        subsample=0.8,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    ),\n",
    "    'Fraction_Frameshifts': lgb.LGBMRegressor(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=8,\n",
    "        num_leaves=15,\n",
    "        min_child_samples=30,\n",
    "        subsample=0.8,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "# Create and train bagged models for each target\n",
    "bagged_models = {}\n",
    "for target, base_model in base_models.items():\n",
    "    print(f\"\\nTraining bagged model for {target}\")\n",
    "    \n",
    "    # Create bagging wrapper\n",
    "    bagged_model = BaggingRegressor(\n",
    "        base_estimator=base_model,\n",
    "        n_estimators=15,  # increased from 10 for more robust ensemble\n",
    "        max_samples=0.8,  # each bag uses 80% of samples\n",
    "        max_features=0.8, # each bag uses 80% of features\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    bagged_model.fit(X_train, y_train[target])\n",
    "    \n",
    "    # Store model\n",
    "    bagged_models[target] = bagged_model\n",
    "    \n",
    "    # Calculate validation score\n",
    "    val_pred = bagged_model.predict(X_val)\n",
    "    val_score = r2_score(y_val[target], val_pred)\n",
    "    print(f\"Validation R² score: {val_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions with all models\n",
    "def predict_all(X):\n",
    "    predictions = {}\n",
    "    for target, model in bagged_models.items():\n",
    "        predictions[target] = model.predict(X)\n",
    "    return predictions\n",
    "\n",
    "# Get validation predictions\n",
    "val_predictions = predict_all(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print overall validation performance\n",
    "print(\"\\nOverall Validation Performance:\")\n",
    "for target in base_models.keys():\n",
    "    score = r2_score(y_val[target], val_predictions[target])\n",
    "    print(f\"{target}: R² = {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "# For each target and model\n",
    "for target, bagged_model in bagged_models.items():\n",
    "    print(f\"\\nFeature Importance for {target}\")\n",
    "    \n",
    "    # Get the number of features in the original dataset\n",
    "    n_features = len(feature_names)\n",
    "    \n",
    "    # Initialize importance array\n",
    "    importance_scores = np.zeros(n_features)\n",
    "    \n",
    "    # Count how many times each feature is used\n",
    "    feature_counts = np.zeros(n_features)\n",
    "    \n",
    "    # Sum importance from all base estimators\n",
    "    for estimator in bagged_model.estimators_:\n",
    "        # Get the selected feature indices for this estimator\n",
    "        feature_indices = bagged_model.estimators_features_[\n",
    "            bagged_model.estimators_.index(estimator)\n",
    "        ]\n",
    "        \n",
    "        # Add importance scores for used features\n",
    "        importance_scores[feature_indices] += estimator.feature_importances_\n",
    "        feature_counts[feature_indices] += 1\n",
    "    \n",
    "    # Average the scores (avoiding division by zero)\n",
    "    feature_counts = np.maximum(feature_counts, 1)  # Avoid division by zero\n",
    "    importance_scores /= feature_counts\n",
    "    \n",
    "    # Create and sort importance DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importance_scores\n",
    "    })\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False).head(20)\n",
    "    importance_df['Importance_Percentage'] = (importance_df['Importance'] / importance_df['Importance'].sum() * 100).round(2)\n",
    "    \n",
    "    # Display results\n",
    "    print(importance_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all importances to a single Excel file with multiple sheets\n",
    "with pd.ExcelWriter('all_feature_importances.xlsx') as writer:\n",
    "   for target, bagged_model in bagged_models.items():\n",
    "       n_features = len(X_train.columns)\n",
    "       importance_scores = np.zeros(n_features)\n",
    "       feature_counts = np.zeros(n_features)\n",
    "       \n",
    "       for estimator in bagged_model.estimators_:\n",
    "           feature_indices = bagged_model.estimators_features_[\n",
    "               bagged_model.estimators_.index(estimator)\n",
    "           ]\n",
    "           importance_scores[feature_indices] += estimator.feature_importances_\n",
    "           feature_counts[feature_indices] += 1\n",
    "       \n",
    "       feature_counts = np.maximum(feature_counts, 1)\n",
    "       importance_scores /= feature_counts\n",
    "       \n",
    "       importance_df = pd.DataFrame({\n",
    "           'Feature': X_train.columns,\n",
    "           'Importance': importance_scores\n",
    "       }).sort_values('Importance', ascending=False)\n",
    "       \n",
    "       importance_df.to_excel(writer, sheet_name=target, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
